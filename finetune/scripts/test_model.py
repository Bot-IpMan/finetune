#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ñ–∞–π–Ω-—Ç—é–Ω–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ
"""

import torch
import json
import time
from pathlib import Path
from typing import List, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

class ModelTester:
    """–ö–ª–∞—Å –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ñ–∞–π–Ω-—Ç—é–Ω–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ"""
    
    def __init__(self, model_path: str = "/workspace/models/finetuned"):
        self.model_path = Path(model_path)
        self.model = None
        self.tokenizer = None
        self.base_model = None
        
    def load_model(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ñ–∞–π–Ω-—Ç—é–Ω–µ–Ω—É –º–æ–¥–µ–ª—å"""
        print("üöÄ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–Ω-—Ç—é–Ω–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ...")
        
        try:
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ pad_token —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å
            base_model = AutoModelForCausalLM.from_pretrained(
                "Qwen/Qwen2.5-Coder-1.5B-Instruct",
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä
            self.model = PeftModel.from_pretrained(base_model, self.model_path)
            
            print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ!")
            return True
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
            return False
    
    def load_base_model(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è"""
        print("üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –±–∞–∑–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è...")
        
        try:
            self.base_model = AutoModelForCausalLM.from_pretrained(
                "Qwen/Qwen2.5-Coder-1.5B-Instruct",
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            print("‚úÖ –ë–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞!")
            return True
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –±–∞–∑–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ: {e}")
            return False
    
    def generate_code(self, instruction: str, input_text: str = "", model=None, max_length: int = 256):
        """–ì–µ–Ω–µ—Ä—É—î –∫–æ–¥ –∑–∞ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—î—é"""
        if model is None:
            model = self.model
            
        # –§–æ—Ä–º—É—î–º–æ –ø—Ä–æ–º–ø—Ç
        if input_text and input_text.strip():
            prompt = f"Instruction: {instruction}\nInput: {input_text}\nOutput:"
        else:
            prompt = f"Instruction: {instruction}\nOutput:"
        
        # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True,
            max_length=512
        ).to(model.device)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è
        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=128,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        generation_time = time.time() - start_time
        
        # –î–µ–∫–æ–¥—É–≤–∞–Ω–Ω—è
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # –í–∏—Ç—è–≥—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π –∫–æ–¥
        if "Output:" in generated_text:
            output = generated_text.split("Output:")[-1].strip()
        else:
            output = generated_text.replace(prompt, "").strip()
        
        return {
            "output": output,
            "generation_time": generation_time,
            "prompt_length": len(inputs["input_ids"][0]),
            "output_length": len(outputs[0]) - len(inputs["input_ids"][0])
        }
    
    def run_test_suite(self):
        """–ó–∞–ø—É—Å–∫–∞—î –Ω–∞–±—ñ—Ä —Ç–µ—Å—Ç—ñ–≤"""
        print("\nüß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä—É...")
        print("=" * 60)
        
        test_cases = [
            {
                "name": "–ë–∞–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è",
                "instruction": "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü—ñ—é –¥–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –¥–≤–æ—Ö —á–∏—Å–µ–ª",
                "input": "Python",
                "expected_keywords": ["def", "return", "+"]
            },
            {
                "name": "–§—É–Ω–∫—Ü—ñ—è –º–Ω–æ–∂–µ–Ω–Ω—è",
                "instruction": "–°—Ç–≤–æ—Ä–∏ —Ñ—É–Ω–∫—Ü—ñ—é –¥–ª—è –º–Ω–æ–∂–µ–Ω–Ω—è –¥–≤–æ—Ö —á–∏—Å–µ–ª",
                "input": "Python",
                "expected_keywords": ["def", "return", "*"]
            },
            {
                "name": "–§—É–Ω–∫—Ü—ñ—è —Ñ–∞–∫—Ç–æ—Ä—ñ–∞–ª—É",
                "instruction": "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü—ñ—é –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è —Ñ–∞–∫—Ç–æ—Ä—ñ–∞–ª—É",
                "input": "",
                "expected_keywords": ["def", "factorial", "return"]
            },
            {
                "name": "–ö–ª–∞—Å –±–∞–Ω–∫—ñ–≤—Å—å–∫–∏–π —Ä–∞—Ö—É–Ω–æ–∫",
                "instruction": "–°—Ç–≤–æ—Ä–∏ –∫–ª–∞—Å –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –±–∞–Ω–∫—ñ–≤—Å—å–∫–∏–º —Ä–∞—Ö—É–Ω–∫–æ–º",
                "input": "Python",
                "expected_keywords": ["class", "def", "self"]
            },
            {
                "name": "–ì—ñ–¥—Ä–æ–ø–æ–Ω–Ω–∏–π –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥",
                "instruction": "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü—ñ—é –¥–ª—è —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É pH —Ä—ñ–≤–Ω—è –¥–ª—è –≥—ñ–¥—Ä–æ–ø–æ–Ω—ñ–∫–∏",
                "input": "Python",
                "expected_keywords": ["def", "ph", "return"]
            }
        ]
        
        results = []
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nüîπ –¢–µ—Å—Ç {i}: {test_case['name']}")
            print(f"   –Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è: {test_case['instruction']}")
            print(f"   –í—Ö—ñ–¥: {test_case['input']}")
            
            # –¢–µ—Å—Ç—É—î–º–æ —Ñ–∞–π–Ω-—Ç—é–Ω–µ–Ω—É –º–æ–¥–µ–ª—å
            finetuned_result = self.generate_code(
                test_case['instruction'], 
                test_case['input']
            )
            
            # –û—Ü—ñ–Ω—é—î–º–æ —è–∫—ñ—Å—Ç—å
            quality_score = self.evaluate_output(
                finetuned_result['output'], 
                test_case['expected_keywords']
            )
            
            print(f"   ‚è±Ô∏è  –ß–∞—Å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {finetuned_result['generation_time']:.2f}s")
            print(f"   üìè –î–æ–≤–∂–∏–Ω–∞ –≤–∏—Ö–æ–¥—É: {finetuned_result['output_length']} —Ç–æ–∫–µ–Ω—ñ–≤")
            print(f"   ‚≠ê –û—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ: {quality_score}/10")
            print(f"   üìù –†–µ–∑—É–ª—å—Ç–∞—Ç:\n{finetuned_result['output'][:200]}...")
            
            # –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –±–∞–∑–æ–≤–æ—é –º–æ–¥–µ–ª–ª—é (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∞)
            if self.base_model is not None:
                base_result = self.generate_code(
                    test_case['instruction'], 
                    test_case['input'],
                    model=self.base_model
                )
                base_quality = self.evaluate_output(
                    base_result['output'], 
                    test_case['expected_keywords']
                )
                
                improvement = quality_score - base_quality
                print(f"   üìä –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –±–∞–∑–æ–≤–æ—é: {improvement:+.1f} –±–∞–ª—ñ–≤")
            
            results.append({
                "test_name": test_case['name'],
                "instruction": test_case['instruction'],
                "input": test_case['input'],
                "output": finetuned_result['output'],
                "quality_score": quality_score,
                "generation_time": finetuned_result['generation_time'],
                "output_length": finetuned_result['output_length']
            })
            
            print("-" * 40)
        
        return results
    
    def evaluate_output(self, output: str, expected_keywords: List[str]) -> float:
        """–û—Ü—ñ–Ω—é—î —è–∫—ñ—Å—Ç—å –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ–≥–æ –∫–æ–¥—É"""
        score = 0.0
        output_lower = output.lower()
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –æ—á—ñ–∫—É–≤–∞–Ω–∏—Ö –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ (40% –æ—Ü—ñ–Ω–∫–∏)
        keyword_score = sum(1 for keyword in expected_keywords if keyword.lower() in output_lower)
        score += (keyword_score / len(expected_keywords)) * 4.0
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–Ω–æ—ó —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ (30% –æ—Ü—ñ–Ω–∫–∏)
        if "def " in output_lower:
            score += 1.0
        if "return" in output_lower:
            score += 1.0
        if output.count("(") == output.count(")"):  # –ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω—ñ –¥—É–∂–∫–∏
            score += 1.0
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –∞–±–æ docstring (20% –æ—Ü—ñ–Ω–∫–∏)
        if '"""' in output or "'''" in output or "#" in output:
            score += 2.0
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–æ–≤–∂–∏–Ω–∏ —Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ (10% –æ—Ü—ñ–Ω–∫–∏)
        if 50 <= len(output) <= 500:  # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞
            score += 1.0
        
        return min(score, 10.0)  # –ú–∞–∫—Å–∏–º—É–º 10 –±–∞–ª—ñ–≤
    
    def generate_report(self, results: List[Dict[str, Any]]):
        """–ì–µ–Ω–µ—Ä—É—î –∑–≤—ñ—Ç –ø—Ä–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è"""
        print("\nüìä –ó–í–Ü–¢ –ü–†–û –¢–ï–°–¢–£–í–ê–ù–ù–Ø")
        print("=" * 60)
        
        # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        avg_quality = sum(r['quality_score'] for r in results) / len(results)
        avg_time = sum(r['generation_time'] for r in results) / len(results)
        avg_length = sum(r['output_length'] for r in results) / len(results)
        
        print(f"üìà –°–µ—Ä–µ–¥–Ω—è –æ—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ: {avg_quality:.1f}/10")
        print(f"‚è±Ô∏è  –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {avg_time:.2f}s")
        print(f"üìè –°–µ—Ä–µ–¥–Ω—è –¥–æ–≤–∂–∏–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: {avg_length:.0f} —Ç–æ–∫–µ–Ω—ñ–≤")
        
        # –†–æ–∑–ø–æ–¥—ñ–ª –æ—Ü—ñ–Ω–æ–∫
        excellent = sum(1 for r in results if r['quality_score'] >= 8)
        good = sum(1 for r in results if 6 <= r['quality_score'] < 8)
        fair = sum(1 for r in results if 4 <= r['quality_score'] < 6)
        poor = sum(1 for r in results if r['quality_score'] < 4)
        
        print(f"\nüèÜ –†–æ–∑–ø–æ–¥—ñ–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤:")
        print(f"   –í—ñ–¥–º—ñ–Ω–Ω–æ (8-10): {excellent}/{len(results)} —Ç–µ—Å—Ç—ñ–≤")
        print(f"   –î–æ–±—Ä–µ (6-8): {good}/{len(results)} —Ç–µ—Å—Ç—ñ–≤")
        print(f"   –ó–∞–¥–æ–≤—ñ–ª—å–Ω–æ (4-6): {fair}/{len(results)} —Ç–µ—Å—Ç—ñ–≤")
        print(f"   –ü–æ–≥–∞–Ω–æ (0-4): {poor}/{len(results)} —Ç–µ—Å—Ç—ñ–≤")
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É
        report_path = Path("/workspace/models/finetuned/test_results.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump({
                "summary": {
                    "avg_quality": avg_quality,
                    "avg_time": avg_time,
                    "avg_length": avg_length,
                    "total_tests": len(results)
                },
                "results": results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –î–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {report_path}")
        
        return avg_quality

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    tester = ModelTester()
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    if not tester.load_model():
        print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å. –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
        return
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –±–∞–∑–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
    print("\nü§î –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è? (–º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ –¥–æ–¥–∞—Ç–∫–æ–≤–∏–π —á–∞—Å)")
    load_base = input("–í–≤–µ–¥—ñ—Ç—å 'y' —â–æ–± –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ (–∞–±–æ –Ω–∞—Ç–∏—Å–Ω—ñ—Ç—å Enter –¥–ª—è –ø—Ä–æ–ø—É—Å–∫—É): ").strip().lower()
    if load_base == 'y':
        tester.load_base_model()
    else:
        print("‚ÑπÔ∏è  –ë–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")

    results = tester.run_test_suite()
    tester.generate_report(results)

if __name__ == "__main__":
    main()
