#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥—É Qwen2.5-Coder –º–æ–¥–µ–ª—ñ
"""

import os
import json
import yaml
import torch
import logging
from pathlib import Path
from typing import Dict, Any, List
from dataclasses import dataclass, field

import transformers
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from datasets import Dataset, load_dataset
import numpy as np

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

@dataclass
class ModelArguments:
    """–ê—Ä–≥—É–º–µ–Ω—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª—ñ"""
    model_name: str = field(default="Qwen/Qwen2.5-Coder-1.5B-Instruct")
    torch_dtype: str = field(default="float16")
    device_map: str = field(default="auto")
    load_in_8bit: bool = field(default=False)
    load_in_4bit: bool = field(default=True)

@dataclass
class LoraArguments:
    """–ê—Ä–≥—É–º–µ–Ω—Ç–∏ –¥–ª—è LoRA"""
    r: int = field(default=16)
    lora_alpha: int = field(default=32)
    lora_dropout: float = field(default=0.1)
    bias: str = field(default="none")
    target_modules: List[str] = field(default_factory=lambda: [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ])

@dataclass
class DataArguments:
    """–ê—Ä–≥—É–º–µ–Ω—Ç–∏ –¥–ª—è –¥–∞–Ω–∏—Ö"""
    train_file: str = field(default="/workspace/data/processed/train_dataset.jsonl")
    eval_file: str = field(default="/workspace/data/processed/eval_dataset.jsonl")
    max_length: int = field(default=512)
    instruction_template: str = field(
        default="Instruction: {instruction}\nInput: {input}\nOutput: {output}"
    )

class CodeDataset:
    """–ö–ª–∞—Å –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –¥–∞—Ç–∞—Å–µ—Ç–æ–º –∫–æ–¥—É"""
    
    def __init__(self, tokenizer, max_length: int = 512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def load_data(self, file_path: str) -> Dataset:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –¥–∞–Ω—ñ –∑ JSONL —Ñ–∞–π–ª—É"""
        try:
            dataset = load_dataset('json', data_files=file_path)['train']
            logger.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(dataset)} –∑—Ä–∞–∑–∫—ñ–≤ –∑ {file_path}")
            return dataset
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {e}")
            raise
    
    def preprocess_function(self, examples):
        """–ü–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö"""
        inputs = []
        for i in range(len(examples['instruction'])):
            instruction = examples['instruction'][i]
            input_text = examples.get('input', [''] * len(examples['instruction']))[i]
            output = examples['output'][i]
            
            # –§–æ—Ä–º—É—î–º–æ –ø—Ä–æ–º–ø—Ç
            if input_text and input_text.strip():
                prompt = f"Instruction: {instruction}\nInput: {input_text}\nOutput: {output}"
            else:
                prompt = f"Instruction: {instruction}\nOutput: {output}"
            
            inputs.append(prompt)
        
        # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è
        model_inputs = self.tokenizer(
            inputs,
            max_length=self.max_length,
            truncation=True,
            padding=False,
            return_tensors=None
        )
        
        # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ labels = input_ids –¥–ª—è causal LM
        model_inputs["labels"] = model_inputs["input_ids"].copy()
        
        return model_inputs

class ModelTrainer:
    """–ö–ª–∞—Å –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
    
    def __init__(self, config_path: str):
        self.config = self.load_config(config_path)
        self.model = None
        self.tokenizer = None
        self.trainer = None
        
    def load_config(self, config_path: str) -> Dict[str, Any]:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def setup_tokenizer(self):
        """–ù–∞–ª–∞—à—Ç–æ–≤—É—î —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä"""
        logger.info("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config['model']['name'],
            trust_remote_code=True,
            padding_side="right"
        )
        
        # –î–æ–¥–∞—î–º–æ pad_token —è–∫—â–æ –π–æ–≥–æ –Ω–µ–º–∞—î
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        logger.info(f"–¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ. Vocab size: {len(self.tokenizer)}")
        
    def setup_model(self):
        """–ù–∞–ª–∞—à—Ç–æ–≤—É—î –º–æ–¥–µ–ª—å"""
        logger.info("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –±–∞–∑–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ...")
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
        model_kwargs = {
            "trust_remote_code": True,
            "torch_dtype": getattr(torch, self.config['model']['torch_dtype']),
            "device_map": self.config['model']['device_map'],
            "low_cpu_mem_usage": True,
        }
        
        if self.config['model']['load_in_4bit']:
            from transformers import BitsAndBytesConfig
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True
            )
            model_kwargs["quantization_config"] = bnb_config
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config['model']['name'],
            **model_kwargs
        )
        
        # –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ LoRA
        lora_config = LoraConfig(
            r=self.config['lora']['r'],
            lora_alpha=self.config['lora']['lora_alpha'],
            target_modules=self.config['lora']['target_modules'],
            lora_dropout=self.config['lora']['lora_dropout'],
            bias=self.config['lora']['bias'],
            task_type=TaskType.CAUSAL_LM,
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
        # Resize token embeddings —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        self.model.resize_token_embeddings(len(self.tokenizer))
        
        logger.info("–ú–æ–¥–µ–ª—å –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∞ –∑ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏")
    
    def prepare_datasets(self):
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤"""
        logger.info("–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤...")
        
        dataset_processor = CodeDataset(self.tokenizer, self.config['data']['max_length'])
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ
        train_dataset = dataset_processor.load_data(self.config['data']['train_file'])
        eval_dataset = dataset_processor.load_data(self.config['data']['eval_file'])
        
        # –û–±—Ä–æ–±–ª—è—î–º–æ –¥–∞–Ω—ñ
        train_dataset = train_dataset.map(
            dataset_processor.preprocess_function,
            batched=True,
            remove_columns=train_dataset.column_names
        )
        
        eval_dataset = eval_dataset.map(
            dataset_processor.preprocess_function,
            batched=True,
            remove_columns=eval_dataset.column_names
        )
        
        logger.info(f"Train dataset: {len(train_dataset)} –∑—Ä–∞–∑–∫—ñ–≤")
        logger.info(f"Eval dataset: {len(eval_dataset)} –∑—Ä–∞–∑–∫—ñ–≤")
        
        return train_dataset, eval_dataset
    
    def setup_trainer(self, train_dataset, eval_dataset):
        """–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ç—Ä–µ–Ω–µ—Ä–∞"""
        logger.info("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ç—Ä–µ–Ω–µ—Ä–∞...")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        training_args = TrainingArguments(
            output_dir=self.config['training']['output_dir'],
            num_train_epochs=self.config['training']['num_train_epochs'],
            per_device_train_batch_size=self.config['training']['per_device_train_batch_size'],
            per_device_eval_batch_size=self.config['training']['per_device_eval_batch_size'],
            gradient_accumulation_steps=self.config['training']['gradient_accumulation_steps'],
            learning_rate=self.config['training']['learning_rate'],
            weight_decay=self.config['training']['weight_decay'],
            warmup_steps=self.config['training']['warmup_steps'],
            logging_steps=self.config['training']['logging_steps'],
            save_steps=self.config['training']['save_steps'],
            eval_steps=self.config['training']['eval_steps'],
            evaluation_strategy=self.config['training']['evaluation_strategy'],
            save_total_limit=self.config['training']['save_total_limit'],
            load_best_model_at_end=self.config['training']['load_best_model_at_end'],
            metric_for_best_model=self.config['training']['metric_for_best_model'],
            greater_is_better=self.config['training']['greater_is_better'],
            dataloader_num_workers=self.config['training']['dataloader_num_workers'],
            remove_unused_columns=self.config['training']['remove_unused_columns'],
            optim=self.config['training']['optim'],
            fp16=self.config['training']['fp16'],
            gradient_checkpointing=self.config['training']['gradient_checkpointing'],
            report_to=self.config['training']['report_to'],
            logging_dir=self.config['logging']['tensorboard_dir']
        )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # Causal LM, –Ω–µ masked LM
        )
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ —Ç—Ä–µ–Ω–µ—Ä
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
        )
        
        logger.info("–¢—Ä–µ–Ω–µ—Ä –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π")
    
    def train(self):
        """–ó–∞–ø—É—Å–∫–∞—î —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è"""
        logger.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è...")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
        os.makedirs(self.config['training']['output_dir'], exist_ok=True)
        os.makedirs(self.config['logging']['tensorboard_dir'], exist_ok=True)
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
        self.setup_tokenizer()
        self.setup_model()
        train_dataset, eval_dataset = self.prepare_datasets()
        self.setup_trainer(train_dataset, eval_dataset)
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        result = self.trainer.train()
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        logger.info("üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
        self.trainer.save_model()
        self.tokenizer.save_pretrained(self.config['training']['output_dir'])
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫
        with open(f"{self.config['training']['output_dir']}/training_results.json", 'w') as f:
            json.dump(result.metrics, f, indent=2)
        
        logger.info("‚úÖ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        return result

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–§–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ Qwen2.5-Coder")
    parser.add_argument(
        "--config",
        default="/workspace/configs/training_config.yaml",
        help="–®–ª—è—Ö –¥–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π–Ω–æ–≥–æ —Ñ–∞–π–ª—É"
    )
    
    args = parser.parse_args()
    
    # –ó–∞–ø—É—Å–∫–∞—î–º–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
    trainer = ModelTrainer(args.config)
    result = trainer.train()
    
    print(f"üéâ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {trainer.config['training']['output_dir']}")

if __name__ == "__main__":
    main()