#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —É—Å–ø—ñ—à–Ω–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è Qwen2.5-Coder –º–æ–¥–µ–ª—ñ
"""

import os
import json
import torch
import logging
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import numpy as np

logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

class TrainingValidator:
    def __init__(self, model_path):
        self.model_path = model_path
        self.results = {}
        
    def check_files_exist(self):
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤"""
        logger.info("üîç –ü–µ—Ä–µ–≤—ñ—Ä—è—é –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ñ–∞–π–ª—ñ–≤ –º–æ–¥–µ–ª—ñ...")
        
        required_files = [
            'adapter_config.json',
            'adapter_model.safetensors',
            'tokenizer_config.json',
            'tokenizer.json',
            'results.json',
            'training_config.json'
        ]
        
        missing_files = []
        existing_files = []
        
        for file in required_files:
            file_path = os.path.join(self.model_path, file)
            if os.path.exists(file_path):
                existing_files.append(file)
                logger.info(f"‚úÖ {file} –∑–Ω–∞–π–¥–µ–Ω–æ")
            else:
                missing_files.append(file)
                logger.warning(f"‚ö†Ô∏è {file} –≤—ñ–¥—Å—É—Ç–Ω—ñ–π")
        
        self.results['files_check'] = {
            'existing_files': existing_files,
            'missing_files': missing_files,
            'all_files_present': len(missing_files) == 0
        }
        
        return len(missing_files) == 0
    
    def check_training_results(self):
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è"""
        logger.info("üìä –ê–Ω–∞–ª—ñ–∑—É—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è...")
        
        results_file = os.path.join(self.model_path, 'results.json')
        config_file = os.path.join(self.model_path, 'training_config.json')
        
        if not os.path.exists(results_file):
            logger.error("‚ùå –§–∞–π–ª results.json –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            return False
            
        try:
            with open(results_file, 'r') as f:
                training_results = json.load(f)
                
            with open(config_file, 'r') as f:
                training_config = json.load(f)
                
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–ª—é—á–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
            train_loss = training_results.get('train_loss', float('inf'))
            train_steps = training_results.get('train_steps_per_second', 0)
            
            logger.info(f"üìà –í—Ç—Ä–∞—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è: {train_loss:.4f}")
            logger.info(f"‚ö° –ö—Ä–æ–∫—ñ–≤ –∑–∞ —Å–µ–∫—É–Ω–¥—É: {train_steps:.2f}")
            
            # –ö—Ä–∏—Ç–µ—Ä—ñ—ó —É—Å–ø—ñ—à–Ω–æ—Å—Ç—ñ
            success_criteria = {
                'loss_reasonable': train_loss < 10.0,  # Loss –Ω–µ –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –∑–∞–Ω–∞–¥—Ç–æ –≤–∏—Å–æ–∫–∏–º
                'loss_not_nan': not np.isnan(train_loss) and not np.isinf(train_loss),
                'steps_completed': train_steps > 0
            }
            
            self.results['training_metrics'] = {
                'train_loss': train_loss,
                'train_steps_per_second': train_steps,
                'success_criteria': success_criteria,
                'training_successful': all(success_criteria.values())
            }
            
            # –í–∏–≤–æ–¥–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            if all(success_criteria.values()):
                logger.info("‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –ø—Ä–æ–π—à–ª–æ —É—Å–ø—ñ—à–Ω–æ!")
            else:
                logger.warning("‚ö†Ô∏è –ú–æ–∂–ª–∏–≤—ñ –ø—Ä–æ–±–ª–µ–º–∏ –∑ –Ω–∞–≤—á–∞–Ω–Ω—è–º:")
                for criterion, passed in success_criteria.items():
                    if not passed:
                        logger.warning(f"   - {criterion}: FAILED")
            
            return all(success_criteria.values())
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤: {e}")
            return False
    
    def test_model_loading(self):
        """–¢–µ—Å—Ç—É—î –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ"""
        logger.info("üîÑ –¢–µ—Å—Ç—É—é –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ...")
        
        try:
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
            tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            logger.info("‚úÖ –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å
            config_file = os.path.join(self.model_path, 'training_config.json')
            with open(config_file, 'r') as f:
                config = json.load(f)
            
            base_model_name = config.get('model_name', 'gpt2')
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å –∑ –∞–¥–∞–ø—Ç–µ—Ä–æ–º
            try:
                # –°–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —è–∫ PEFT –º–æ–¥–µ–ª—å
                base_model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=True
                )
                model = PeftModel.from_pretrained(base_model, self.model_path)
                logger.info("‚úÖ PEFT –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
                peft_model = True
                
            except Exception as peft_error:
                logger.warning(f"PEFT –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏–ª–∞—Å—å: {peft_error}")
                # –°–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —è–∫ –∑–≤–∏—á–∞–π–Ω—É –º–æ–¥–µ–ª—å
                model = AutoModelForCausalLM.from_pretrained(self.model_path)
                logger.info("‚úÖ –ó–≤–∏—á–∞–π–Ω–∞ –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
                peft_model = False
            
            self.results['model_loading'] = {
                'tokenizer_loaded': True,
                'model_loaded': True,
                'is_peft_model': peft_model,
                'base_model': base_model_name
            }
            
            return model, tokenizer
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
            self.results['model_loading'] = {
                'tokenizer_loaded': False,
                'model_loaded': False,
                'error': str(e)
            }
            return None, None
    
    def test_model_inference(self, model, tokenizer):
        """–¢–µ—Å—Ç—É—î –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é —Ç–µ–∫—Å—Ç—É –Ω–∞–≤—á–µ–Ω–æ—é –º–æ–¥–µ–ª–ª—é"""
        logger.info("üß™ –¢–µ—Å—Ç—É—é –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é —Ç–µ–∫—Å—Ç—É...")
        
        test_prompts = [
            "def hello():",
            "class Calculator:",
            "import numpy as np\n\ndef process_data(",
            "# This function calculates"
        ]
        
        generation_results = []
        
        try:
            model.eval()
            
            for i, prompt in enumerate(test_prompts):
                logger.info(f"–¢–µ—Å—Ç {i+1}: '{prompt}'")
                
                # –¢–æ–∫–µ–Ω—ñ–∑—É—î–º–æ –≤—Ö—ñ–¥
                inputs = tokenizer(prompt, return_tensors="pt", padding=True)
                
                # –ì–µ–Ω–µ—Ä—É—î–º–æ —Ç–µ–∫—Å—Ç
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=50,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                # –î–µ–∫–æ–¥—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
                new_text = generated[len(prompt):].strip()
                
                result = {
                    'prompt': prompt,
                    'generated': new_text,
                    'full_output': generated,
                    'success': len(new_text) > 0
                }
                
                generation_results.append(result)
                logger.info(f"‚úÖ –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ: '{new_text[:50]}...'")
            
            successful_generations = sum(1 for r in generation_results if r['success'])
            
            self.results['inference_test'] = {
                'total_tests': len(test_prompts),
                'successful_generations': successful_generations,
                'success_rate': successful_generations / len(test_prompts),
                'results': generation_results,
                'inference_working': successful_generations > 0
            }
            
            logger.info(f"üìä –£—Å–ø—ñ—à–Ω–∏—Ö –≥–µ–Ω–µ—Ä–∞—Ü—ñ–π: {successful_generations}/{len(test_prompts)}")
            
            return successful_generations > 0
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {e}")
            self.results['inference_test'] = {
                'error': str(e),
                'inference_working': False
            }
            return False
    
    def check_model_size(self):
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —Ä–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ —Ç–∞ –∞–¥–∞–ø—Ç–µ—Ä—ñ–≤"""
        logger.info("üìè –ü–µ—Ä–µ–≤—ñ—Ä—è—é —Ä–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—ñ–≤ –º–æ–¥–µ–ª—ñ...")
        
        file_sizes = {}
        total_size = 0
        
        for file in os.listdir(self.model_path):
            file_path = os.path.join(self.model_path, file)
            if os.path.isfile(file_path):
                size = os.path.getsize(file_path)
                file_sizes[file] = size
                total_size += size
                
                size_mb = size / (1024 * 1024)
                logger.info(f"üìÑ {file}: {size_mb:.2f} MB")
        
        total_mb = total_size / (1024 * 1024)
        logger.info(f"üìä –ó–∞–≥–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä: {total_mb:.2f} MB")
        
        self.results['model_size'] = {
            'file_sizes': file_sizes,
            'total_size_bytes': total_size,
            'total_size_mb': total_mb,
            'reasonable_size': total_mb > 0.1  # –ü—Ä–∏–Ω–∞–π–º–Ω—ñ 100KB
        }
        
        return total_mb > 0.1
    
    def run_full_validation(self):
        """–ó–∞–ø—É—Å–∫–∞—î –ø–æ–≤–Ω—É –≤–∞–ª—ñ–¥–∞—Ü—ñ—é"""
        logger.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –ø–æ–≤–Ω–æ—ó –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ...")
        
        validation_steps = [
            ("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ñ–∞–π–ª—ñ–≤", self.check_files_exist),
            ("–ê–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –Ω–∞–≤—á–∞–Ω–Ω—è", self.check_training_results),
            ("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É –º–æ–¥–µ–ª—ñ", self.check_model_size),
        ]
        
        success_count = 0
        
        for step_name, step_func in validation_steps:
            logger.info(f"\n--- {step_name} ---")
            try:
                success = step_func()
                if success:
                    success_count += 1
                    logger.info(f"‚úÖ {step_name}: –ü–†–û–ô–î–ï–ù–û")
                else:
                    logger.warning(f"‚ö†Ô∏è {step_name}: –ü–†–û–ë–õ–ï–ú–ò")
            except Exception as e:
                logger.error(f"‚ùå {step_name}: –ü–û–ú–ò–õ–ö–ê - {e}")
        
        # –¢–µ—Å—Ç—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å
        logger.info(f"\n--- –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ ---")
        model, tokenizer = self.test_model_loading()
        
        if model is not None and tokenizer is not None:
            success_count += 1
            inference_success = self.test_model_inference(model, tokenizer)
            if inference_success:
                success_count += 1
        
        # –ü—ñ–¥—Å—É–º–∫–∏
        total_steps = len(validation_steps) + 2  # +2 –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É
        success_rate = success_count / total_steps
        
        self.results['overall'] = {
            'total_validation_steps': total_steps,
            'successful_steps': success_count,
            'success_rate': success_rate,
            'validation_passed': success_rate >= 0.8  # 80% —É—Å–ø—ñ—Ö—É
        }
        
        logger.info(f"\nüéØ –ü–Ü–î–°–£–ú–ö–ò –í–ê–õ–Ü–î–ê–¶–Ü–á:")
        logger.info(f"   –£—Å–ø—ñ—à–Ω–∏—Ö –∫—Ä–æ–∫—ñ–≤: {success_count}/{total_steps}")
        logger.info(f"   –†—ñ–≤–µ–Ω—å —É—Å–ø—ñ—Ö—É: {success_rate*100:.1f}%")
        
        if success_rate >= 0.8:
            logger.info("üéâ –ù–ê–í–ß–ê–ù–ù–Ø –ü–†–û–ô–®–õ–û –£–°–ü–Ü–®–ù–û!")
        else:
            logger.warning("‚ö†Ô∏è –í–ò–Ø–í–õ–ï–ù–û –ü–†–û–ë–õ–ï–ú–ò –ó –ù–ê–í–ß–ê–ù–ù–Ø–ú")
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        validation_results_path = os.path.join(self.model_path, 'validation_results.json')
        with open(validation_results_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        logger.info(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {validation_results_path}")
        
        return success_rate >= 0.8

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    import sys
    
    if len(sys.argv) != 2:
        print("–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è: python validate_training.py <—à–ª—è—Ö_–¥–æ_–Ω–∞–≤—á–µ–Ω–æ—ó_–º–æ–¥–µ–ª—ñ>")
        print("–ü—Ä–∏–∫–ª–∞–¥: python validate_training.py ./output")
        sys.exit(1)
    
    model_path = sys.argv[1]
    
    if not os.path.exists(model_path):
        logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è {model_path} –Ω–µ —ñ—Å–Ω—É—î")
        sys.exit(1)
    
    validator = TrainingValidator(model_path)
    success = validator.run_full_validation()
    
    if success:
        print("\n‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –ø—Ä–æ–π—à–ª–æ —É—Å–ø—ñ—à–Ω–æ! –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è.")
        sys.exit(0)
    else:
        print("\n‚ùå –í–∏—è–≤–ª–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–∏ –∑ –Ω–∞–≤—á–∞–Ω–Ω—è–º. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –ª–æ–≥–∏ –≤–∏—â–µ.")
        sys.exit(1)

if __name__ == "__main__":
    main()