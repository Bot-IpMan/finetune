#!/usr/bin/env python3
"""
–í–∏–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ñ–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥—É Qwen2.5-Coder –º–æ–¥–µ–ª—ñ
–í–∏–ø—Ä–∞–≤–ª–µ–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏ –∑ scipy —Ç–∞ bitsandbytes
"""

import os
import json
import torch
import logging
import tempfile
import shutil
from pathlib import Path

# –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –∑–º—ñ–Ω–Ω—ñ –æ—Ç–æ—á–µ–Ω–Ω—è –ü–ï–†–ï–î —ñ–º–ø–æ—Ä—Ç–æ–º transformers
temp_base = tempfile.mkdtemp(prefix='qwen_train_')
os.environ['HF_HOME'] = temp_base
os.environ['TRANSFORMERS_CACHE'] = temp_base
os.environ['HF_DATASETS_CACHE'] = temp_base
os.environ['HUGGINGFACE_HUB_CACHE'] = temp_base

# –í–∏–º–∏–∫–∞—î–º–æ bitsandbytes –¥–ª—è CPU
os.environ['DISABLE_BNB'] = '1'
os.environ['BNB_DISABLE_APEX'] = '1'

import transformers
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

class CodeTrainer:
    def __init__(self):
        self.temp_dir = temp_base
        self.output_dir = os.path.join(self.temp_dir, 'output')
        
        logger.info(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é —Ç–∏–º—á–∞—Å–æ–≤–∏–π –∫–∞—Ç–∞–ª–æ–≥: {self.temp_dir}")
        
        self.config = {
            'model_name': 'Qwen/Qwen2.5-Coder-1.5B',  # –ú–µ–Ω—à–∞ –º–æ–¥–µ–ª—å –¥–ª—è CPU
            'max_length': 128,  # –©–µ –±—ñ–ª—å—à–µ —Å–∫–æ—Ä–æ—á–µ–Ω–æ –¥–ª—è CPU
            'batch_size': 1,
            'learning_rate': 1e-4,
            'epochs': 1,
            'lora_r': 2,  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è CPU
            'lora_alpha': 4,
        }
        
    def create_dummy_dataset(self):
        """–°—Ç–≤–æ—Ä—é—î –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π —Ç–µ—Å—Ç–æ–≤–∏–π –¥–∞—Ç–∞—Å–µ—Ç"""
        data = [
            {"text": "def hello():\n    print('Hello World')"},
            {"text": "def add(a, b):\n    return a + b"},
            {"text": "class Calculator:\n    def multiply(self, x, y):\n        return x * y"},
            {"text": "import numpy as np\n\ndef process_array(arr):\n    return np.mean(arr)"},
        ]
        return Dataset.from_list(data)
    
    def setup_tokenizer(self):
        """–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞"""
        logger.info("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞...")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config['model_name'],
                trust_remote_code=True,
                cache_dir=self.temp_dir,
                local_files_only=False,
                force_download=False,
                use_fast=True  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —à–≤–∏–¥–∫–∏–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
            )
            logger.info(f"–£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –¥–ª—è {self.config['model_name']}")
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ {self.config['model_name']}: {e}")
            # Fallback –¥–æ CodeT5 –∞–±–æ GPT2
            fallback_models = ["Salesforce/codet5-base", "gpt2"]
            
            for fallback in fallback_models:
                try:
                    logger.info(f"–ü—Ä–æ–±—É—é fallback —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä: {fallback}")
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        fallback,
                        cache_dir=self.temp_dir,
                        use_fast=True
                    )
                    self.config['model_name'] = fallback
                    logger.info(f"–£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ fallback —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä: {fallback}")
                    break
                except Exception as fallback_error:
                    logger.warning(f"Fallback {fallback} —Ç–∞–∫–æ–∂ –Ω–µ –ø—Ä–∞—Ü—é—î: {fallback_error}")
                    continue
            else:
                raise RuntimeError("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∂–æ–¥–µ–Ω —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä")
            
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            logger.info("–í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ pad_token —è–∫ eos_token")
            
    def setup_model(self):
        """–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
        logger.info("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
        
        try:
            # –°–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å –ë–ï–ó quantization
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config['model_name'],
                torch_dtype=torch.float32,  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ float32 –¥–ª—è CPU
                device_map=None,  # –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ device_map –¥–ª—è CPU
                trust_remote_code=True,
                cache_dir=self.temp_dir,
                low_cpu_mem_usage=True,
                # –í–∏–º–∏–∫–∞—î–º–æ –≤—Å—ñ quantization –æ–ø—Ü—ñ—ó
                load_in_8bit=False,
                load_in_4bit=False,
                quantization_config=None
            )
            logger.info(f"–£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –º–æ–¥–µ–ª—å: {self.config['model_name']}")
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {self.config['model_name']}: {e}")
            # Fallback –¥–æ –º–µ–Ω—à–æ—ó –¥–æ—Å—Ç—É–ø–Ω–æ—ó –º–æ–¥–µ–ª—ñ
            fallback_models = ["gpt2", "distilgpt2"]
            
            for fallback in fallback_models:
                try:
                    logger.info(f"–ü—Ä–æ–±—É—é fallback –º–æ–¥–µ–ª—å: {fallback}")
                    self.model = AutoModelForCausalLM.from_pretrained(
                        fallback,
                        cache_dir=self.temp_dir,
                        torch_dtype=torch.float32,
                        low_cpu_mem_usage=True
                    )
                    self.config['model_name'] = fallback
                    logger.info(f"–£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ fallback –º–æ–¥–µ–ª—å: {fallback}")
                    break
                except Exception as fallback_error:
                    logger.warning(f"Fallback {fallback} —Ç–∞–∫–æ–∂ –Ω–µ –ø—Ä–∞—Ü—é—î: {fallback_error}")
                    continue
            else:
                raise RuntimeError("–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∂–æ–¥–Ω—É –º–æ–¥–µ–ª—å")
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ target_modules –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª—ñ
        model_type = self.model.config.model_type.lower()
        logger.info(f"–¢–∏–ø –º–æ–¥–µ–ª—ñ: {model_type}")
        
        if "gpt" in model_type:
            target_modules = ["c_attn", "c_proj"]
        elif "qwen" in model_type:
            target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
        elif "codet5" in model_type:
            target_modules = ["q", "v", "k", "o"]
        else:
            # –ó–∞–≥–∞–ª—å–Ω—ñ –Ω–∞–∑–≤–∏ –¥–ª—è transformer –º–æ–¥–µ–ª–µ–π
            target_modules = ["q_proj", "v_proj"]
            logger.warning(f"–ù–µ–≤—ñ–¥–æ–º–∏–π —Ç–∏–ø –º–æ–¥–µ–ª—ñ {model_type}, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é –∑–∞–≥–∞–ª—å–Ω—ñ target_modules")
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è LoRA –ë–ï–ó quantization
        try:
            lora_config = LoraConfig(
                r=self.config['lora_r'],
                lora_alpha=self.config['lora_alpha'],
                target_modules=target_modules,
                lora_dropout=0.1,
                bias="none",
                task_type=TaskType.CAUSAL_LM,
                # –í–∏–º–∏–∫–∞—î–º–æ –≤—Å—ñ quantization –æ–ø—Ü—ñ—ó
                use_rslora=False,
                init_lora_weights=True,
            )
            
            self.model = get_peft_model(self.model, lora_config)
            logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø—ñ—à–Ω–æ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∞ –∑ LoRA")
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è LoRA: {e}")
            # –Ø–∫—â–æ LoRA –Ω–µ –ø—Ä–∞—Ü—é—î, –ø—Ä–∞—Ü—é—î–º–æ –±–µ–∑ –Ω–µ—ó
            logger.warning("–ü—Ä–∞—Ü—é—é –±–µ–∑ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞")
            # –ó–∞–º–æ—Ä–æ–∂—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ –¥–ª—è –µ–∫–æ–Ω–æ–º—ñ—ó –ø–∞–º'—è—Ç—ñ
            for param in self.model.parameters():
                param.requires_grad = False
            # –†–æ–∑–º–æ—Ä–æ–∂—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—ñ —à–∞—Ä–∏
            for param in list(self.model.parameters())[-4:]:
                param.requires_grad = True
        
    def tokenize_data(self, examples):
        """–¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö"""
        result = self.tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=self.config['max_length'],
            return_tensors=None,
        )
        result["labels"] = result["input_ids"].copy()
        return result
    
    def train(self):
        """–ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è"""
        logger.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è...")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –≤–∏—Ö—ñ–¥–Ω—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é
        os.makedirs(self.output_dir, exist_ok=True)
        
        try:
            # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
            self.setup_tokenizer()
            self.setup_model()
            
            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
            dataset = self.create_dummy_dataset()
            tokenized_dataset = dataset.map(
                self.tokenize_data,
                batched=True,
                remove_columns=dataset.column_names,
                num_proc=1  # –û–¥–∏–Ω –ø—Ä–æ—Ü–µ—Å –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
            )
            
            logger.info(f"–†–æ–∑–º—ñ—Ä –¥–∞—Ç–∞—Å–µ—Ç—É: {len(tokenized_dataset)}")
            
            # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –¥–ª—è CPU
            training_args = TrainingArguments(
                output_dir=self.output_dir,
                num_train_epochs=self.config['epochs'],
                per_device_train_batch_size=self.config['batch_size'],
                learning_rate=self.config['learning_rate'],
                warmup_steps=1,
                logging_steps=1,
                save_steps=50,
                save_total_limit=1,
                dataloader_num_workers=0,
                remove_unused_columns=False,
                report_to=[],  # –ë–µ–∑ –∑–≤—ñ—Ç–Ω–æ—Å—Ç—ñ
                logging_dir=None,
                disable_tqdm=False,
                # CPU –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
                dataloader_pin_memory=False,
                gradient_checkpointing=False,  # –í–∏–º–∏–∫–∞—î–º–æ –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
                fp16=False,  # –í–∏–º–∏–∫–∞—î–º–æ FP16 –¥–ª—è CPU
                bf16=False,  # –í–∏–º–∏–∫–∞—î–º–æ BF16 –¥–ª—è CPU
                optim="adamw_torch",  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π optimizer
            )
            
            # Data collator
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False,
                pad_to_multiple_of=None,  # –í–∏–º–∏–∫–∞—î–º–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç–∏
            )
            
            # –¢—Ä–µ–Ω–µ—Ä
            trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=tokenized_dataset,
                tokenizer=self.tokenizer,
                data_collator=data_collator,
            )
            
            # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
            logger.info("–ü–æ—á–∞—Ç–æ–∫ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è...")
            result = trainer.train()
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
            logger.info("üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
            trainer.save_model()
            self.tokenizer.save_pretrained(self.output_dir)
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            with open(os.path.join(self.output_dir, 'results.json'), 'w') as f:
                json.dump(result.metrics, f, indent=2)
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
            with open(os.path.join(self.output_dir, 'training_config.json'), 'w') as f:
                json.dump(self.config, f, indent=2)
            
            logger.info(f"‚úÖ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞ –≤ {self.output_dir}")
            logger.info(f"üìä –í—Ç—Ä–∞—Ç–∏: {result.training_loss:.4f}")
            
            return result
            
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {e}")
            import traceback
            traceback.print_exc()
            raise
        
        finally:
            # –û—á–∏—â–µ–Ω–Ω—è –ø–∞–º'—è—Ç—ñ
            if hasattr(self, 'model'):
                del self.model
            if hasattr(self, 'tokenizer'):
                del self.tokenizer
            torch.cuda.empty_cache() if torch.cuda.is_available() else None

def install_missing_packages():
    """–í—Å—Ç–∞–Ω–æ–≤–ª—é—î –≤—ñ–¥—Å—É—Ç–Ω—ñ –ø–∞–∫–µ—Ç–∏"""
    import subprocess
    import sys
    
    required_packages = [
        'scipy',  # –û—Å–Ω–æ–≤–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞
        'datasets',
        'peft',
        'accelerate'
    ]
    
    for package in required_packages:
        try:
            __import__(package)
            logger.info(f"‚úÖ {package} –≤–∂–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        except ImportError:
            logger.info(f"üì¶ –í—Å—Ç–∞–Ω–æ–≤–ª—é—é {package}...")
            try:
                subprocess.check_call([sys.executable, "-m", "pip", "install", package])
                logger.info(f"‚úÖ {package} —É—Å–ø—ñ—à–Ω–æ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            except subprocess.CalledProcessError as e:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è {package}: {e}")

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    try:
        # –°–ø–æ—á–∞—Ç–∫—É –≤—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ –ø–∞–∫–µ—Ç–∏
        logger.info("üîç –ü–µ—Ä–µ–≤—ñ—Ä—è—é —Ç–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª—é—é –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –ø–∞–∫–µ—Ç–∏...")
        install_missing_packages()
        
        # –ü–æ–∫–∞–∑—É—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ
        logger.info(f"Python: {torch.__version__}")
        logger.info(f"Transformers: {transformers.__version__}")
        logger.info(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
        logger.info(f"–ü—Ä–∏—Å—Ç—Ä—ñ–π: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
        
        trainer = CodeTrainer()
        result = trainer.train()
        
        print("üéâ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {trainer.output_dir}")
        
    except KeyboardInterrupt:
        print("‚ùå –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –ø–µ—Ä–µ—Ä–≤–∞–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        
        # –ü—Ä–æ–ø–æ–Ω—É—î–º–æ —Ä—ñ—à–µ–Ω–Ω—è
        print("\nüîß –ú–æ–∂–ª–∏–≤—ñ —Ä—ñ—à–µ–Ω–Ω—è:")
        print("1. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å scipy: pip install scipy")
        print("2. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å –≤—Å—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ: pip install scipy datasets peft accelerate")
        print("3. –Ø–∫—â–æ –ø—Ä–æ–±–ª–µ–º–∏ –∑ bitsandbytes: pip uninstall bitsandbytes")
        print("4. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –≤—ñ—Ä—Ç—É–∞–ª—å–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ –∑ Python 3.8+")

if __name__ == "__main__":
    main()