#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è —Ñ–∞–π–Ω-—Ç—é–Ω–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ
"""

import os
import json
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
from dataclasses import dataclass
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from datasets import Dataset, load_dataset
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class EvaluationMetrics:
    """–ö–ª–∞—Å –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è"""
    accuracy: float = 0.0
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    perplexity: float = 0.0
    bleu_score: float = 0.0
    code_quality_score: float = 0.0

class ModelEvaluator:
    """–ö–ª–∞—Å –¥–ª—è –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
    
    def __init__(self, model_path: str = "/workspace/models/finetuned"):
        self.model_path = Path(model_path)
        self.model = None
        self.tokenizer = None
        self.base_model = None
        self.results = []
        
    def load_models(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ñ–∞–π–Ω-—Ç—é–Ω–µ–Ω—É —Ç–∞ –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—ñ"""
        logger.info("üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π...")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–Ω-—Ç—é–Ω–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ
        base_model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen2.5-Coder-1.5B-Instruct",
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        self.model = PeftModel.from_pretrained(base_model, self.model_path)
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –±–∞–∑–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        self.base_model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen2.5-Coder-1.5B-Instruct",
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        logger.info("‚úÖ –ú–æ–¥–µ–ª—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
    
    def calculate_perplexity(self, model, dataset: Dataset) -> float:
        """–û–±—á–∏—Å–ª—é—î perplexity –º–æ–¥–µ–ª—ñ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç—ñ"""
        logger.info("üìä –û–±—á–∏—Å–ª–µ–Ω–Ω—è perplexity...")
        
        model.eval()
        total_loss = 0
        total_tokens = 0
        
        with torch.no_grad():
            for example in dataset:
                # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è
                inputs = self.tokenizer(
                    example['text'] if 'text' in example else str(example),
                    return_tensors="pt",
                    max_length=512,
                    truncation=True,
                    padding=True
                ).to(model.device)
                
                # –û–±—á–∏—Å–ª–µ–Ω–Ω—è loss
                outputs = model(**inputs, labels=inputs["input_ids"])
                loss = outputs.loss
                
                total_loss += loss.item() * inputs["input_ids"].size(1)
                total_tokens += inputs["input_ids"].size(1)
        
        perplexity = torch.exp(torch.tensor(total_loss / total_tokens))
        return perplexity.item()
    
    def evaluate_code_quality(self, generated_code: str) -> Dict[str, float]:
        """–û—Ü—ñ–Ω—é—î —è–∫—ñ—Å—Ç—å –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ–≥–æ –∫–æ–¥—É"""
        scores = {
            'syntax_score': 0.0,
            'structure_score': 0.0,
            'completeness_score': 0.0,
            'readability_score': 0.0
        }
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å—É
        try:
            compile(generated_code, '<string>', 'exec')
            scores['syntax_score'] = 1.0
        except SyntaxError:
            scores['syntax_score'] = 0.0
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏
        if 'def ' in generated_code:
            scores['structure_score'] += 0.3
        if 'class ' in generated_code:
            scores['structure_score'] += 0.3
        if 'return' in generated_code:
            scores['structure_score'] += 0.2
        if generated_code.count('(') == generated_code.count(')'):
            scores['structure_score'] += 0.2
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–æ–≤–Ω–æ—Ç–∏
        if len(generated_code.strip()) > 20:
            scores['completeness_score'] += 0.3
        if len(generated_code.split('\n')) > 3:
            scores['completeness_score'] += 0.3
        if any(keyword in generated_code for keyword in ['def', 'class', 'import']):
            scores['completeness_score'] += 0.4
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ
        if '"""' in generated_code or "'''" in generated_code:
            scores['readability_score'] += 0.4
        if '#' in generated_code:
            scores['readability_score'] += 0.3
        if len([line for line in generated_code.split('\n') if line.strip()]) > 0:
            scores['readability_score'] += 0.3
        
        return scores
    
    def comprehensive_evaluation(self, eval_dataset_path: str):
        """–ü—Ä–æ–≤–æ–¥–∏—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω–µ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è"""
        logger.info("üéØ –ü–æ—á–∞—Ç–æ–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è...")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
        eval_dataset = load_dataset('json', data_files=eval_dataset_path)['train']
        
        finetuned_results = []
        base_results = []
        
        for i, example in enumerate(eval_dataset):
            if i >= 50:  # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–ª—è —à–≤–∏–¥—à–æ–≥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
                break
                
            instruction = example['instruction']
            input_text = example.get('input', '')
            expected_output = example['output']
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ñ–∞–π–Ω-—Ç—é–Ω–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ
            finetuned_output = self.generate_response(self.model, instruction, input_text)
            finetuned_quality = self.evaluate_code_quality(finetuned_output)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –±–∞–∑–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ
            base_output = self.generate_response(self.base_model, instruction, input_text)
            base_quality = self.evaluate_code_quality(base_output)
            
            finetuned_results.append({
                'instruction': instruction,
                'expected': expected_output,
                'generated': finetuned_output,
                'quality_scores': finetuned_quality
            })
            
            base_results.append({
                'instruction': instruction,
                'expected': expected_output,
                'generated': base_output,
                'quality_scores': base_quality
            })
            
            if (i + 1) % 10 == 0:
                logger.info(f"–û–±—Ä–æ–±–ª–µ–Ω–æ {i + 1} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤...")
        
        return finetuned_results, base_results
    
    def generate_response(self, model, instruction: str, input_text: str = "") -> str:
        """–ì–µ–Ω–µ—Ä—É—î –≤—ñ–¥–ø–æ–≤—ñ–¥—å –º–æ–¥–µ–ª—ñ"""
        if input_text and input_text.strip():
            prompt = f"Instruction: {instruction}\nInput: {input_text}\nOutput:"
        else:
            prompt = f"Instruction: {instruction}\nOutput:"
        
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            max_length=512,
            truncation=True,
            padding=True
        ).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=128,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # –í–∏—Ç—è–≥—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π –∫–æ–¥
        if "Output:" in generated_text:
            output = generated_text.split("Output:")[-1].strip()
        else:
            output = generated_text.replace(prompt, "").strip()
        
        return output
    
    def calculate_aggregate_metrics(self, results: List[Dict]) -> EvaluationMetrics:
        """–û–±—á–∏—Å–ª—é—î –∞–≥—Ä–µ–≥–æ–≤–∞–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏"""
        metrics = EvaluationMetrics()
        
        if not results:
            return metrics
        
        # –°–µ—Ä–µ–¥–Ω—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏ —è–∫–æ—Å—Ç—ñ –∫–æ–¥—É
        syntax_scores = [r['quality_scores']['syntax_score'] for r in results]
        structure_scores = [r['quality_scores']['structure_score'] for r in results]
        completeness_scores = [r['quality_scores']['completeness_score'] for r in results]
        readability_scores = [r['quality_scores']['readability_score'] for r in results]
        
        metrics.accuracy = np.mean(syntax_scores)
        metrics.precision = np.mean(structure_scores)
        metrics.recall = np.mean(completeness_scores)
        metrics.f1_score = np.mean(readability_scores)
        
        # –ó–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ –∫–æ–¥—É
        total_quality = []
        for r in results:
            quality = np.mean(list(r['quality_scores'].values()))
            total_quality.append(quality)
        
        metrics.code_quality_score = np.mean(total_quality)
        
        return metrics
    
    def generate_comparison_report(self, finetuned_results: List[Dict], base_results: List[Dict]):
        """–ì–µ–Ω–µ—Ä—É—î –ø–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∏–π –∑–≤—ñ—Ç"""
        logger.info("üìã –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É...")
        
        finetuned_metrics = self.calculate_aggregate_metrics(finetuned_results)
        base_metrics = self.calculate_aggregate_metrics(base_results)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–≤—ñ—Ç
        report = {
            'summary': {
                'total_examples_evaluated': len(finetuned_results),
                'finetuned_model_performance': {
                    'syntax_accuracy': finetuned_metrics.accuracy,
                    'structure_score': finetuned_metrics.precision,
                    'completeness_score': finetuned_metrics.recall,
                    'readability_score': finetuned_metrics.f1_score,
                    'overall_code_quality': finetuned_metrics.code_quality_score
                },
                'base_model_performance': {
                    'syntax_accuracy': base_metrics.accuracy,
                    'structure_score': base_metrics.precision,
                    'completeness_score': base_metrics.recall,
                    'readability_score': base_metrics.f1_score,
                    'overall_code_quality': base_metrics.code_quality_score
                },
                'improvements': {
                    'syntax_improvement': finetuned_metrics.accuracy - base_metrics.accuracy,
                    'structure_improvement': finetuned_metrics.precision - base_metrics.precision,
                    'completeness_improvement': finetuned_metrics.recall - base_metrics.recall,
                    'readability_improvement': finetuned_metrics.f1_score - base_metrics.f1_score,
                    'overall_improvement': finetuned_metrics.code_quality_score - base_metrics.code_quality_score
                }
            },
            'detailed_results': {
                'finetuned_examples': finetuned_results[:10],  # –ü–µ—Ä—à—ñ 10 –¥–ª—è –ø—Ä–∏–∫–ª–∞–¥—É
                'base_examples': base_results[:10]
            }
        }
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
        report_path = self.model_path / "evaluation_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {report_path}")
        
        # –í–∏–≤–µ–¥–µ–Ω–Ω—è —Ä–µ–∑—é–º–µ
        self.print_summary(report['summary'])
        
        return report
    
    def print_summary(self, summary: Dict):
        """–í–∏–≤–æ–¥–∏—Ç—å —Ä–µ–∑—é–º–µ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è"""
        print("\n" + "="*60)
        print("üìä –†–ï–ó–Æ–ú–ï –û–¶–Ü–ù–Æ–í–ê–ù–ù–Ø –ú–û–î–ï–õ–Ü")
        print("="*60)
        
        print(f"üîç –û—Ü—ñ–Ω–µ–Ω–æ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤: {summary['total_examples_evaluated']}")
        print("\nüéØ –§–ê–ô–ù-–¢–Æ–ù–ï–ù–ê –ú–û–î–ï–õ–¨:")
        ft_perf = summary['finetuned_model_performance']
        print(f"  –°–∏–Ω—Ç–∞–∫—Å–∏—á–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å: {ft_perf['syntax_accuracy']:.3f}")
        print(f"  –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞ –æ—Ü—ñ–Ω–∫–∞: {ft_perf['structure_score']:.3f}")
        print(f"  –ü–æ–≤–Ω–æ—Ç–∞ –∫–æ–¥—É: {ft_perf['completeness_score']:.3f}")
        print(f"  –ß–∏—Ç–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å: {ft_perf['readability_score']:.3f}")
        print(f"  üèÜ –ó–∞–≥–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å: {ft_perf['overall_code_quality']:.3f}")
        
        print("\nüìö –ë–ê–ó–û–í–ê –ú–û–î–ï–õ–¨:")
        base_perf = summary['base_model_performance']
        print(f"  –°–∏–Ω—Ç–∞–∫—Å–∏—á–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å: {base_perf['syntax_accuracy']:.3f}")
        print(f"  –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞ –æ—Ü—ñ–Ω–∫–∞: {base_perf['structure_score']:.3f}")
        print(f"  –ü–æ–≤–Ω–æ—Ç–∞ –∫–æ–¥—É: {base_perf['completeness_score']:.3f}")
        print(f"  –ß–∏—Ç–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å: {base_perf['readability_score']:.3f}")
        print(f"  üèÜ –ó–∞–≥–∞–ª—å–Ω–∞ —è–∫—ñ—Å—Ç—å: {base_perf['overall_code_quality']:.3f}")
        
        print("\nüìà –ü–û–ö–†–ê–©–ï–ù–ù–Ø:")
        improvements = summary['improvements']
        print(f"  –°–∏–Ω—Ç–∞–∫—Å–∏—Å: {improvements['syntax_improvement']:+.3f}")
        print(f"  –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {improvements['structure_improvement']:+.3f}")
        print(f"  –ü–æ–≤–Ω–æ—Ç–∞: {improvements['completeness_improvement']:+.3f}")
        print(f"  –ß–∏—Ç–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å: {improvements['readability_improvement']:+.3f}")
        print(f"  üéâ –ó–∞–≥–∞–ª—å–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è: {improvements['overall_improvement']:+.3f}")
        
        if improvements['overall_improvement'] > 0:
            print("\n‚úÖ –§–∞–π–Ω-—Ç—é–Ω—ñ–Ω–≥ –£–°–ü–Ü–®–ù–ò–ô! –ú–æ–¥–µ–ª—å –ø–æ–∫—Ä–∞—â–∏–ª–∞—Å—å.")
        else:
            print("\n‚ö†Ô∏è –ü–æ—Ç—Ä—ñ–±–Ω–µ –¥–æ–¥–∞—Ç–∫–æ–≤–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è. –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–Ω–∏—Ö –ø–æ–∫—Ä–∞—â–µ–Ω—å.")

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    evaluator = ModelEvaluator()
    
    try:
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
        evaluator.load_models()
        
        # –ö–æ–º–ø–ª–µ–∫—Å–Ω–µ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è
        finetuned_results, base_results = evaluator.comprehensive_evaluation(
            "/workspace/data/processed/eval_dataset.jsonl"
        )
        
        # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–≤—ñ—Ç—É
        report = evaluator.generate_comparison_report(finetuned_results, base_results)
        
        print("\nüéâ –û—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
        
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è: {e}")
        raise

if __name__ == "__main__":
    main()
